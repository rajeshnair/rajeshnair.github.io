{"name":"Rajeshnair.GitHub.io","tagline":"","body":"# Machine Learning\r\n\r\n## Linear Regression with one variable\r\n\r\n### Hypothesis function \r\nHypothesis function tries to provide a model for the training data to predict future values\r\n\r\n````\r\nh(x) = p1 + p2*x\r\n````\r\n\r\nwhere p1 is dedcued parameter 1 and p2 is parameter 2\r\n\r\n### Cost function\r\nCost function is a function to measure the cost of a given hypothesis in order to ascertain the validity of hypothesis. A hypothesis with 0 cost means it perfectly describes the training data for all the data points.\r\nHigher the cost means more inaccurate is the hypothesis. Our aim would be to come up with minimize the cost by coming upo with better parameters.\r\n\r\n````\r\nCost(p1,p2) = 1/2m * sum(sqr(h(x) - y))\r\n````\r\nor\r\n````\r\nCost(p1,p2) = 1/2m * sum(sqr((p1 + p2*x) - y))\r\n````\r\nwhere x is independent variable and y is dependent variable \r\n \r\n### Gradient descent \r\nGiven that we have hypothesis and a a way to measure its accuracy (cost function), we now need a mechanism to improve hypothesis. This is done by gradient descent by \r\n\r\n## Linear Regression with multiple variable\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}